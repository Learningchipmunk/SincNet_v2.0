[data]
;If you are using DCASE2018 task2 data, do not change this
tr_lst=data_lists/Tensor_Training_list.npy
;If you are using DCASE2018 task2 data, do not change this
te_lst=data_lists/Tensor_Validation_list.npy
;If you are using DCASE2018 task2 data, do not change this
lab_dict=data_lists/DCASE_tensor_train_labels.npy
;The path to your preprocessed Data
data_folder=Data/Audio_Tensors/Train/Preprocessed_withEnergy_AudioTensors_Window5000ms_32kHz_Random0Padding/
;The path where you wish to store your models.
output_folder=exp/SincNet_DCASE_v2.0/test2D
;If you wish to load a previous model, indicate the path here.
pt_file=none

[windowing]
;Sampling Rate you used for preprocessing
fs=32000
;Window size in ms, this is the size in ms of the input of the net
cw_len=4000
;Window shift in ms (or Hop Length), it is the size of the shift in ms between each inputs in the accuracy function.
cw_shift=400

;Architecture of our best 2D CNN model:
[cnn2D]
;Number of filters by layer
cnn_N_filt=64,30,60,90
;Filter Width for each layer
cnn_len_filt_W=251,3,3,3
;Filter Height for each layer
cnn_len_filt_H=0,3,3,3
;Energy is computed with average pooling, cnn_energy_L is the length of the average pooling
cnn_energy_L=2048
;Stride of average pooling
cnn_energy_stride=1024
;Max pooling Width for each layer
cnn_max_pool_len_W=1,2,2,2
;Max pooling Height for each layer
cnn_max_pool_len_H=1,2,2,2
;Indicates if we use layer normalization on input, True means we do
cnn_use_laynorm_inp=True
;Indicates if we use batch normalization on input, True means we do
cnn_use_batchnorm_inp=False
;Indicates if we use layer normalization after each layer before dropout, True means we do
cnn_use_laynorm=False,False,False,False
;Indicates if we use layer normalization after each layer before dropout, True means we do
cnn_use_batchnorm=True,True,True,True
cnn_act=leaky_relu,leaky_relu,leaky_relu,leaky_relu
;Drop proportion for each layer
cnn_drop=0.0,0.0,0.0,0.0

;Architecture of our best DNN model accordingly:
[dnn]
fc_lay=512,256,256
fc_drop=0.3,0.3,0.3
fc_use_laynorm_inp=True
fc_use_batchnorm_inp=False
fc_use_batchnorm=True,True,True
fc_use_laynorm=False,False,False
fc_act=leaky_relu,leaky_relu,leaky_relu

;Architecture of our best classifier model:
[class]
;DCASE2018 task2 data has 41 classes, to change if you are using different data
class_lay=41
use_scheduler=True
class_drop=0.0
class_use_laynorm_inp=False
class_use_batchnorm_inp=False
class_use_laynorm=False
class_use_batchnorm=False
class_act=softmax

;Hyper parameters and Data augmentation parameters
[optimization]
;    You can check the different optimizers here: https://ruder.io/optimizing-gradient-descent/
;User can choose the optimizer he wishes to use between [`RMSprop`, `Adamax`, `Adam`]:
optimizer_type=RMSprop
;The initial value of the learning rate, whatever scheduler you will use the lr won't exceed this value:
lr=0.001 

;   Inspired by `Cyclical Learning Rates for Training Neural Networks`
;                             Leslie N. Smith
;                  https://arxiv.org/pdf/1506.01186.pdf

;Indicates if the user wishes to use a scheduler, it should be set to True if so.
use_scheduler=True
;    You can check the different pytorch schedulers here: https://pytorch.org/docs/stable/optim.html
;User can choose the scheduler he wishes to use between [`ReduceLROnPlateau`, `CyclicLR_triangular`, `CyclicLR_triangular2`, `CyclicLR_exp_range`]:
scheduler_type=ReduceLROnPlateau

;               Parameters for ReduceLROnPlateau:
;Number of epochs with no improvement after which learning rate will be reduced. 
;For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, 
;and will only decrease the LR after the 3rd epoch if the loss still hasn’t improved then
scheduler_patience=2
;Is the amount the Learning rate will be reduced if scheduler patience is exceeded:
scheduler_factor=0.5

;Batch size of the training dataset:
batch_size=20
;Batch size of the validation dataset:
Batch_dev=20
;Early stopping patience:
patience=7
;tqdm epoch loop:
N_epochs=100
;The DataSet trainloader amplifies the signal by np.random.uniform(1.0-fact_amp,1+fact_amp):
fact_amp=0.2
;Validation evaluation after N_eval_epoch:
N_eval_epoch=5
;Will print running acc and loss on training set after train_acc_period:
train_acc_period=100

;    Inspired by `mixup: BEYOND EMPIRICAL RISK MINIMIZATION`
;                            Hongyi Zhang
;                https://arxiv.org/pdf/1710.09412.pdf

;Indicates if the user wishes to use a scheduler, it should be set to True if so.
use_mixup=False
;Indicates the proportion of the data the user wishes to mixup, it should be a float in [0,1]
mixup_batch_prop=0
;This is the coeficient of the beta distribution used for the mixup: λ ∼ Beta(beta_coef, beta_coef)
beta_coef=0.4
;If the user chose to use mixup, he can also choose Between Intraclass and interclass mixup.
;If same classes is False and use_mixup is True, the data will be mixed up regardless of their label values.
;If same classes is True and use_mixup is True, only data with same label value will be mixed up together.
same_classes=False

;numpy.random seed is set to:
seed=1234

