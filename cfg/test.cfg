;; Here is the commented cfg that is mentionned in README.md
;; You should be able to adapt this cfg and run it if you followed all the steps mentionned beforehand
;; <!> Please do not forget to change the number of classes `class_lay` in the [class] field <!>

[data]
;If you are using DCASE2018 task2 data, do not change this
tr_lst=data_lists/Tensor_Training_list.npy
;If you are using DCASE2018 task2 data, do not change this
te_lst=data_lists/Tensor_Validation_list.npy
;If you are using DCASE2018 task2 data, do not change this
lab_dict=data_lists/DCASE_tensor_train_labels.npy
;The path to your preprocessed Data
data_folder=Data/Audio_Tensors/Train/Preprocessed_withEnergy_AudioTensors_Window1000ms_Random0Padding/
;The path where you wish to store your models. You should change it after each run!
output_folder=exp/SincNet_DCASE_v2.0/test
;If you wish to load a previous model, indicate the path here.
pt_file=none

[windowing]
;Sampling Rate you used for preprocessing
fs=32000
;Window size in ms, this is the size in ms of the input of the net
cw_len=800
;Window shift in ms (or Hop Length), it is the size of the shift in ms between each inputs in the accuracy function.
cw_shift=80

[cnn]
;Architecture of our best 1D CNN model:
cnn_N_filt=80,60,60,60
cnn_len_filt=251,5,5,5
cnn_max_pool_len=3,3,3,3
;here inp stands for input:
cnn_use_laynorm_inp=True
cnn_use_batchnorm_inp=False
cnn_use_laynorm=True,True,True,True
cnn_use_batchnorm=False,False,False,False
cnn_act=leaky_relu,leaky_relu,leaky_relu,leaky_relu
cnn_drop=0.0,0.0,0.0,0.0

[dnn]
;Architecture of our best DNN model:
fc_lay=1516,1024,512
fc_drop=0.3,0.3,0.3,0.3
fc_use_laynorm_inp=True
fc_use_batchnorm_inp=False
fc_use_batchnorm=True,True,True
fc_use_laynorm=False,False,False
fc_act=prelu,prelu,prelu

[class]
;Architecture of our best classifier model:
;DCASE2018 task2 data has 41 classes, to change if you are using different data
class_lay=41
use_scheduler=True
class_drop=0.0
class_use_laynorm_inp=False
class_use_batchnorm_inp=False
class_use_batchnorm=False
class_use_laynorm=False
class_act=softmax

;Hyper parameters and Data augmentation parameters
[optimization]
;User can choose the optimizer he wishes to use between [`rmsprop`, `adamax`, `adam`]:
optimizer_type=RMSprop
lr=0.001 
use_scheduler=True
scheduler_patience=2
scheduler_factor=0.5
batch_size=32
Batch_dev=32
patience=7
N_epochs=100
;The DataSet trainloader amplifies the signal by np.random.uniform(1.0-fact_amp,1+fact_amp):
fact_amp=0.2
N_eval_epoch=5
train_acc_period=100
use_mixup=False
mixup_batch_prop=0
beta_coef=0.4
same_classes=False
seed=1234
