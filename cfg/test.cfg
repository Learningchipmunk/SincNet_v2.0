;; Here is the commented cfg that is mentionned in README.md
;; You should be able to adapt this cfg and run it if you followed all the steps mentionned beforehand
;; <!> Please do not forget to change the number of classes `class_lay` in the [class] field <!>

[data]
;If you are using DCASE2018 task2 data, do not change this
tr_lst=data_lists/Tensor_Training_list.npy
;If you are using DCASE2018 task2 data, do not change this
te_lst=data_lists/Tensor_Validation_list.npy
;If you are using DCASE2018 task2 data, do not change this
lab_dict=data_lists/DCASE_tensor_train_labels.npy
;The path to your preprocessed Data
data_folder=Data/Audio_Tensors/Train/Preprocessed_withEnergy_AudioTensors_Window1000ms_Random0Padding/
;The path where you wish to store your models. You should change it after each run!
output_folder=exp/SincNet_DCASE_v2.0/test
;If you wish to load a previous model, indicate the path here.
pt_file=none

[windowing]
;Sampling Rate you used for preprocessing
fs=32000
;Window size in ms, this is the size in ms of the input of the net
cw_len=800
;Window shift in ms (or Hop Length), it is the size of the shift in ms between each inputs in the accuracy function.
cw_shift=80

[cnn]
;Architecture of our best 1D CNN model:
cnn_N_filt=80,60,60,60
cnn_len_filt=251,5,5,5
cnn_max_pool_len=3,3,3,3
;here inp stands for input:
cnn_use_laynorm_inp=True
cnn_use_batchnorm_inp=False
cnn_use_laynorm=True,True,True,True
cnn_use_batchnorm=False,False,False,False
cnn_act=leaky_relu,leaky_relu,leaky_relu,leaky_relu
cnn_drop=0.0,0.0,0.0,0.0

[dnn]
;Architecture of our best DNN model:
fc_lay=1516,1024,512
fc_drop=0.3,0.3,0.3,0.3
fc_use_laynorm_inp=True
fc_use_batchnorm_inp=False
fc_use_laynorm=False,False,False
fc_use_batchnorm=True,True,True
fc_act=prelu,prelu,prelu

[class]
;Architecture of our best classifier model:
;DCASE2018 task2 data has 41 classes, to change if you are using different data
class_lay=41
use_scheduler=True
class_drop=0.0
class_use_laynorm_inp=False
class_use_batchnorm_inp=False
class_use_laynorm=False
class_use_batchnorm=False
class_act=softmax

;Hyper parameters and Data augmentation parameters
[optimization]
;    You can check the different optimizers here: https://ruder.io/optimizing-gradient-descent/
;User can choose the optimizer he wishes to use between [`rmsprop`, `adamax`, `adam`]:
optimizer_type=RMSprop
lr=0.001 

;   Inspired by `Cyclical Learning Rates for Training Neural Networks`
;                             Leslie N. Smith
;                  https://arxiv.org/pdf/1506.01186.pdf

;Indicates if the user wishes to use a scheduler, it should be set to True if so.
use_scheduler=True
;    You can check the different pytorch schedulers here: https://pytorch.org/docs/stable/optim.html
;User can choose the scheduler he wishes to use between [`ReduceLROnPlateau`, `CyclicLR`]:
scheduler_type="ReduceLROnPlateau"

;Number of epochs with no improvement after which learning rate will be reduced. 
;For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, 
;and will only decrease the LR after the 3rd epoch if the loss still hasn’t improved then
scheduler_patience=2
;Is the amount the Learning rate will be reduced if scheduler patience is exceeded:
scheduler_factor=0.5
batch_size=32
;Batch size of the validation dataset:
Batch_dev=32
;Early stopping
patience=7
N_epochs=100
;The DataSet trainloader amplifies the signal by np.random.uniform(1.0-fact_amp,1+fact_amp):
fact_amp=0.2
;Validation evaluation after N_eval_epoch:
N_eval_epoch=5
train_acc_period=100

;    Inspired by `mixup: BEYOND EMPIRICAL RISK MINIMIZATION`
;                            Hongyi Zhang
;                https://arxiv.org/pdf/1710.09412.pdf

;Indicates if the user wishes to use a scheduler, it should be set to True if so.
use_mixup=False
;Indicates the proportion of the data the user wishes to mixup, it should be a float in [0,1]
mixup_batch_prop=0
;This is the coeficient of the beta distribution used for the mixup: λ ∼ Beta(beta_coef, beta_coef)
beta_coef=0.4
;If the user chose to use mixup, he can also choose Between Intraclass and interclass mixup.
;If same classes is False and use_mixup is True, the data will be mixed up regardless of their label values.
;If same classes is True and use_mixup is True, only data with same label value will be mixed up together.
same_classes=False

;numpy.random seed is set to:
seed=1234
